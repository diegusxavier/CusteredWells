{"cells":[{"cell_type":"markdown","metadata":{"id":"hfC7pDys1xRf"},"source":["# Objetivo do Notebook\n","Esse notebook tem como objetivo dividir a base de dados das coletas feitas pelo Instituto de Geociências da Unicamp (IG) com base na data em que as coletas foram feitas e aplicar o algoritmo *Self Organizing Maps* (SOM) para obter grupos de poços relativamente semelhantes com base nos dados fornecidos. Para isso, é necessário que a base de dados das coletas esteja tratada."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50699,"status":"ok","timestamp":1690410060024,"user":{"displayName":"Diego Xavier Machado","userId":"02296400187750410274"},"user_tz":180},"id":"-lsJDHVQ1oC2","outputId":"c2220b37-10fe-4c57-b9a2-cfe2e37b35db"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pyproj in /usr/local/lib/python3.10/dist-packages (3.6.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pyproj) (2023.7.22)\n","Requirement already satisfied: folium in /usr/local/lib/python3.10/dist-packages (0.14.0)\n","Requirement already satisfied: branca\u003e=0.6.0 in /usr/local/lib/python3.10/dist-packages (from folium) (0.6.0)\n","Requirement already satisfied: jinja2\u003e=2.9 in /usr/local/lib/python3.10/dist-packages (from folium) (3.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from folium) (1.22.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from folium) (2.27.1)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2\u003e=2.9-\u003efolium) (2.1.3)\n","Requirement already satisfied: urllib3\u003c1.27,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003efolium) (1.26.16)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003efolium) (2023.7.22)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-\u003efolium) (2.0.12)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003efolium) (3.4)\n","Collecting sklearn-som\n","  Downloading sklearn_som-1.1.0-py3-none-any.whl (6.7 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sklearn-som) (1.22.4)\n","Installing collected packages: sklearn-som\n","Successfully installed sklearn-som-1.1.0\n","Collecting git+https://github.com/IanAguiar-ai/metrics.git\n","  Cloning https://github.com/IanAguiar-ai/metrics.git to /tmp/pip-req-build-59u4x89t\n","  Running command git clone --filter=blob:none --quiet https://github.com/IanAguiar-ai/metrics.git /tmp/pip-req-build-59u4x89t\n","  Resolved https://github.com/IanAguiar-ai/metrics.git to commit c7b50f89d88ec2fbec32bdc7f5963ee5b8586e92\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from metrics==0.0.11) (3.7.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from metrics==0.0.11) (1.22.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from metrics==0.0.11) (1.5.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from metrics==0.0.11) (1.2.2)\n","Requirement already satisfied: contourpy\u003e=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003emetrics==0.0.11) (1.1.0)\n","Requirement already satisfied: cycler\u003e=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003emetrics==0.0.11) (0.11.0)\n","Requirement already satisfied: fonttools\u003e=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003emetrics==0.0.11) (4.41.1)\n","Requirement already satisfied: kiwisolver\u003e=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003emetrics==0.0.11) (1.4.4)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003emetrics==0.0.11) (23.1)\n","Requirement already satisfied: pillow\u003e=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003emetrics==0.0.11) (9.4.0)\n","Requirement already satisfied: pyparsing\u003e=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003emetrics==0.0.11) (3.1.0)\n","Requirement already satisfied: python-dateutil\u003e=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003emetrics==0.0.11) (2.8.2)\n","Requirement already satisfied: pytz\u003e=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-\u003emetrics==0.0.11) (2022.7.1)\n","Requirement already satisfied: scipy\u003e=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-\u003emetrics==0.0.11) (1.10.1)\n","Requirement already satisfied: joblib\u003e=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-\u003emetrics==0.0.11) (1.3.1)\n","Requirement already satisfied: threadpoolctl\u003e=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-\u003emetrics==0.0.11) (3.2.0)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil\u003e=2.7-\u003ematplotlib-\u003emetrics==0.0.11) (1.16.0)\n","Building wheels for collected packages: metrics\n","  Building wheel for metrics (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for metrics: filename=metrics-0.0.11-py3-none-any.whl size=17649 sha256=631996d4b1c0f0d77a06b7bbb12387e4223875ed0c6c81e19b86a917dc5eef72\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-2xfdcldr/wheels/c7/48/53/4f50b27b511fdefedb30381ad08615e2dc2bae6ab5a0986841\n","Successfully built metrics\n","Installing collected packages: metrics\n","Successfully installed metrics-0.0.11\n","Collecting git+https://github.com/IanAguiar-ai/Self_Organizing_Maps.git\n","  Cloning https://github.com/IanAguiar-ai/Self_Organizing_Maps.git to /tmp/pip-req-build-tefzuf0a\n","  Running command git clone --filter=blob:none --quiet https://github.com/IanAguiar-ai/Self_Organizing_Maps.git /tmp/pip-req-build-tefzuf0a\n","  Resolved https://github.com/IanAguiar-ai/Self_Organizing_Maps.git to commit 3d5d474cadc8817e3f044c88089b8b159846b856\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from SOM==0.0.10) (3.7.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from SOM==0.0.10) (1.22.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from SOM==0.0.10) (1.5.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from SOM==0.0.10) (1.10.1)\n","Requirement already satisfied: contourpy\u003e=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003eSOM==0.0.10) (1.1.0)\n","Requirement already satisfied: cycler\u003e=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003eSOM==0.0.10) (0.11.0)\n","Requirement already satisfied: fonttools\u003e=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003eSOM==0.0.10) (4.41.1)\n","Requirement already satisfied: kiwisolver\u003e=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003eSOM==0.0.10) (1.4.4)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003eSOM==0.0.10) (23.1)\n","Requirement already satisfied: pillow\u003e=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003eSOM==0.0.10) (9.4.0)\n","Requirement already satisfied: pyparsing\u003e=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003eSOM==0.0.10) (3.1.0)\n","Requirement already satisfied: python-dateutil\u003e=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003eSOM==0.0.10) (2.8.2)\n","Requirement already satisfied: pytz\u003e=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-\u003eSOM==0.0.10) (2022.7.1)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil\u003e=2.7-\u003ematplotlib-\u003eSOM==0.0.10) (1.16.0)\n","Building wheels for collected packages: SOM\n","  Building wheel for SOM (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for SOM: filename=SOM-0.0.10-py3-none-any.whl size=10893 sha256=a40d6eeef1b51094b47d1974b60b6b286cabca634b67dd5b14c11d826da943ea\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-k74xyod2/wheels/48/c1/09/53c9fb0b2d4077feb46d5428bae8548177aa7cd3317945c61d\n","Successfully built SOM\n","Installing collected packages: SOM\n","Successfully installed SOM-0.0.10\n"]}],"source":["#-----------------------------------------------------#\n","#              IMPORTAÇÃO DAS BIBIOTECAS              #\n","#-----------------------------------------------------#\n","\n","!pip install pyproj\n","!pip install folium\n","!pip install sklearn-som\n","!pip install git+https://github.com/IanAguiar-ai/metrics.git\n","!pip install git+https://github.com/IanAguiar-ai/Self_Organizing_Maps.git\n","\n","from pyproj import Proj\n","import folium\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import plotly.express as px\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","from random import seed, random\n","from mutual_information import metrics as mtr\n","import som.som as som\n","from sklearn_som.som import SOM\n","import itertools"]},{"cell_type":"markdown","metadata":{"id":"T-egUqaVlrrV"},"source":["# Importação dos arquivos\n","Para esse notebook, será necessário apenas o arquivo da base de dados das coletas, de forma já tratada, em arquivo `.csv`."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24474,"status":"ok","timestamp":1690410084493,"user":{"displayName":"Diego Xavier Machado","userId":"02296400187750410274"},"user_tz":180},"id":"Vb5U7KuSlshA","outputId":"805290b9-1675-4e8b-c0cc-e54c85a09910"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["#-----------------------------------------------------#\n","#               IMPORTAÇÃO DOS ARQUIVOS               #\n","#-----------------------------------------------------#\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# df = Base de dados\n","df = pd.read_csv('/content/drive/Shareddrives/datasci4water/IG/data/interim/df_final.csv')\n","df = df.reset_index(drop=True)"]},{"cell_type":"markdown","metadata":{"id":"kndyaOMElw2p"},"source":["# Separação da base de dados principal de acordo com a data:\n","Com a base de dados principal, será criado duas outras novas base de dados. Uma para cada período de coleta. Portanto a data é a variável responsável por essa divisão. Assim, formam-se, bases de dados para os seguintes períodos:\n","\n","\n","*   Abril de 2019\n","*   Outubro de 2019\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1690410084494,"user":{"displayName":"Diego Xavier Machado","userId":"02296400187750410274"},"user_tz":180},"id":"4v53GP-TlxY1"},"outputs":[],"source":["#-----------------------------------------------------#\n","#         SEPARAÇÃO DA BASE DE DADOS POR DATA         #\n","#-----------------------------------------------------#\n","\n","# Fazer a separação por data\n","df_abril = df.loc[df['date'] == '2019/04']\n","df_outubro = df.loc[df['date'] == '2019/10']"]},{"cell_type":"markdown","metadata":{"id":"VWeqZ2k5lzwE"},"source":["# Padronização dos valores\n","Como a base de dados possuem valores não numéricos referentes ao poço da amostra (coluna \"*well*\") e referente a data da coleta (coluna \"*date*\"), faremos a separação apenas dos valores numéricos para que o trabalho do algoritmo de aprendizado de máquina seja apenas sobre esses valores numéricos.\n","\n","Ademais, como os valores possuem uma grande diferença entre ordens de grandeza, será feito a padronização desses valores, mantendo-os na mesma escala. Dessa  forma, pode-se evitar que variáveis com altas ordens de grandeza implique no cálculo de grandes distâncias entre os dados e influencie negativamente o funcionamento do algoritmo *K-Means*."]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1690410084495,"user":{"displayName":"Diego Xavier Machado","userId":"02296400187750410274"},"user_tz":180},"id":"PnJjpldBl10b"},"outputs":[],"source":["#-----------------------------------------------------#\n","#               PADRONIZAÇÃO DOS VALORES              #\n","#-----------------------------------------------------#\n","\n","#  Separação dos valores numéricos\n","df_abril_values = df_abril.iloc[:, 2:df_abril.shape[1]]\n","df_outubro_values = df_outubro.iloc[:, 2:df_outubro.shape[1]]\n","\n","# Padronização dos vallores propriamente dito\n","df_scaler = StandardScaler()\n","df_abril_values = df_scaler.fit_transform(df_abril_values)\n","df_outubro_values = df_scaler.fit_transform(df_outubro_values)"]},{"cell_type":"markdown","metadata":{"id":"D8cXI89Al31Y"},"source":["# Mudança no sistema de coordenadas\n","As coordenadas no Dataframe estão no sistema UTM. Para a utilização da biblioteca *folium* no qual serão impressos os rótulos do agrupamento, as coordenadas devem estar no sistema de graus decimais. Para isso, faz-se a conversão dos valores\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1690410084495,"user":{"displayName":"Diego Xavier Machado","userId":"02296400187750410274"},"user_tz":180},"id":"UsL5092fl59C"},"outputs":[],"source":["#-----------------------------------------------------#\n","#                MUDANÇA DE COORDENADAS               #\n","#-----------------------------------------------------#\n","\n","def utm_para_graus_decimais(easting, northing, lista_coordenadas):\n","    zona = 23\n","    hemisferio = 'S'\n","    proj = Proj(proj='utm', zone=zona, south=True, ellps='WGS84')\n","    longitude, latitude = proj(easting, northing, inverse=True)\n","    lista_coordenadas.append([latitude, longitude])\n","\n","\n","coordenadas_abril = []\n","coordenadas_outubro = []\n","\n","for i in range(df_abril.shape[0]):\n","  utm_n = df_abril.iloc[i, 48]\n","  utm_e = df_abril.iloc[i, 47]\n","  utm_para_graus_decimais(utm_e, utm_n, coordenadas_abril)\n","\n","for i in range(df_outubro.shape[0]):\n","  utm_n = df_outubro.iloc[i, 48]\n","  utm_e = df_outubro.iloc[i, 47]\n","  utm_para_graus_decimais(utm_e, utm_n, coordenadas_outubro)"]},{"cell_type":"markdown","metadata":{"id":"BPhp20Pd4QnA"},"source":["# Métricas para o agrupamento do algoritmo SOM\n","Antes de executarmos as métricas, foi necessário resolver corrigir o seguinte problema:\n","\n","  *Devido ao baixo número de amostras e ao grande número de variáveis na base de dados, cada vez em que se executava o algoritmo SOM era possível fazer os grupos diferentes do experimento anterior.*\n","\n","  Para contornar esse problema, para cada matriz m por n de neurônios do algoritmo, são feitos vários experimentos e o agrupamento mais frequente é considerado o melhor agrupamento para aquele matriz de neurônios. Dessa forma, fa-se uso da seguinte função."]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":382,"status":"ok","timestamp":1690410084871,"user":{"displayName":"Diego Xavier Machado","userId":"02296400187750410274"},"user_tz":180},"id":"nR3-znDgBasr"},"outputs":[],"source":["def melhor_agrupamento(dados, n_execucoes, i, j):\n","  resultados = []\n","  for execucao in range(n_execucoes):\n","    # Instanciando e ajustando o SOM aos dados\n","    som = SOM(m=i, n=j, dim=dados.shape[1])\n","    som.fit(dados)\n","    resultados.append(som.predict(dados))\n","\n","  # Contar a frequência de cada conjunto de grupos\n","  frequencias = {}\n","  for grupos in resultados:\n","      grupos_str = ' '.join(map(str, grupos))\n","      if grupos_str in frequencias:\n","          frequencias[grupos_str] += 1\n","      else:\n","          frequencias[grupos_str] = 1\n","  # Encontrar os grupos mais frequentes\n","  grupos_mais_frequentes = max(frequencias, key=frequencias.get)\n","  grupos_mais_frequentes = list(map(int, grupos_mais_frequentes.split()))\n","  print(f\"Matriz SOM {i}x{j}\")\n","  print(grupos_mais_frequentes)\n","  print(\"Frequência:\", max(frequencias.values()), \"| Número de grupos:\", len(set(grupos_mais_frequentes)))\n","  return grupos_mais_frequentes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"56fdG0zx8S6s"},"outputs":[{"name":"stdout","output_type":"stream","text":["Matriz SOM 2x2\n","[1, 1, 0, 3, 0, 3, 1, 3, 3, 1, 1, 0, 0, 3, 1, 3, 0, 2, 2]\n","Frequência: 60 | Número de grupos: 4\n","Matriz SOM 2x3\n","[5, 4, 1, 0, 2, 3, 4, 3, 3, 4, 5, 1, 1, 3, 5, 3, 0, 2, 2]\n","Frequência: 25 | Número de grupos: 6\n","Matriz SOM 2x4\n","[7, 7, 6, 2, 1, 3, 7, 3, 3, 7, 7, 6, 6, 3, 7, 3, 1, 5, 5]\n","Frequência: 7 | Número de grupos: 6\n","Matriz SOM 2x5\n","[9, 9, 8, 3, 2, 4, 9, 4, 4, 9, 9, 8, 8, 4, 9, 4, 3, 7, 7]\n","Frequência: 4 | Número de grupos: 6\n","Matriz SOM 2x6\n","[11, 4, 3, 5, 9, 5, 4, 5, 5, 11, 11, 3, 3, 5, 11, 5, 2, 10, 10]\n","Frequência: 3 | Número de grupos: 7\n","Matriz SOM 3x2\n","[5, 3, 2, 0, 4, 1, 3, 1, 1, 3, 5, 2, 2, 1, 5, 1, 0, 4, 4]\n","Frequência: 27 | Número de grupos: 6\n","Matriz SOM 3x3\n","[6, 3, 4, 1, 7, 0, 3, 0, 0, 3, 6, 4, 4, 0, 6, 0, 2, 7, 7]\n","Frequência: 3 | Número de grupos: 7\n","Matriz SOM 3x4\n","[4, 4, 8, 1, 5, 0, 4, 0, 0, 4, 4, 8, 8, 0, 4, 0, 6, 9, 9]\n","Frequência: 2 | Número de grupos: 7\n","Matriz SOM 3x5\n","[2, 1, 6, 5, 11, 0, 1, 0, 0, 1, 2, 6, 6, 0, 2, 0, 10, 7, 12]\n","Frequência: 2 | Número de grupos: 9\n","Matriz SOM 3x6\n","[10, 4, 5, 6, 8, 12, 4, 12, 12, 4, 3, 5, 5, 12, 3, 12, 13, 15, 9]\n","Frequência: 1 | Número de grupos: 10\n","Matriz SOM 4x2\n","[7, 7, 4, 6, 3, 6, 7, 6, 6, 7, 7, 4, 4, 6, 7, 6, 2, 5, 5]\n","Frequência: 6 | Número de grupos: 6\n","Matriz SOM 4x3\n","[6, 7, 8, 10, 5, 10, 7, 10, 9, 9, 6, 8, 8, 10, 6, 10, 5, 4, 4]\n","Frequência: 2 | Número de grupos: 7\n","Matriz SOM 4x4\n","[12, 8, 14, 5, 15, 9, 8, 9, 10, 13, 13, 14, 14, 9, 13, 9, 5, 0, 11]\n","Frequência: 1 | Número de grupos: 10\n","Matriz SOM 4x5\n","[12, 12, 14, 11, 18, 7, 12, 6, 6, 12, 17, 13, 13, 7, 17, 6, 18, 3, 4]\n","Frequência: 1 | Número de grupos: 10\n","Matriz SOM 4x6\n","[22, 12, 6, 20, 10, 14, 12, 14, 19, 13, 23, 7, 7, 14, 23, 14, 9, 11, 11]\n","Frequência: 1 | Número de grupos: 12\n","Matriz SOM 5x2\n","[0, 2, 3, 5, 1, 4, 2, 4, 4, 2, 0, 3, 3, 4, 0, 4, 5, 1, 1]\n","Frequência: 6 | Número de grupos: 6\n","Matriz SOM 5x3\n","[2, 5, 8, 0, 3, 1, 5, 1, 1, 2, 2, 8, 8, 1, 2, 1, 3, 7, 7]\n","Frequência: 2 | Número de grupos: 7\n","Matriz SOM 5x4\n","[13, 9, 4, 15, 18, 10, 5, 11, 11, 9, 13, 8, 8, 10, 13, 10, 15, 14, 18]\n","Frequência: 1 | Número de grupos: 10\n","Matriz SOM 5x5\n","[17, 18, 19, 23, 24, 21, 18, 21, 22, 22, 12, 18, 18, 21, 17, 21, 23, 16, 11]\n","Frequência: 1 | Número de grupos: 10\n","Matriz SOM 5x6\n","[29, 29, 27, 22, 7, 23, 29, 23, 23, 29, 20, 28, 28, 23, 21, 17, 6, 2, 8]\n","Frequência: 1 | Número de grupos: 12\n","Matriz SOM 6x2\n","[0, 2, 3, 1, 5, 1, 2, 1, 1, 0, 0, 3, 3, 1, 0, 1, 5, 4, 4]\n","Frequência: 4 | Número de grupos: 6\n","Matriz SOM 6x3\n","[12, 13, 14, 16, 11, 16, 13, 16, 15, 15, 12, 17, 17, 16, 12, 16, 11, 10, 10]\n","Frequência: 2 | Número de grupos: 8\n","Matriz SOM 6x4\n","[6, 10, 14, 11, 21, 7, 10, 3, 11, 10, 6, 13, 13, 3, 6, 3, 1, 0, 4]\n","Frequência: 1 | Número de grupos: 11\n","Matriz SOM 6x5\n","[26, 27, 22, 23, 29, 24, 27, 24, 24, 23, 26, 17, 17, 24, 26, 24, 29, 6, 5]\n","Frequência: 1 | Número de grupos: 9\n","Matriz SOM 6x6\n","[14, 8, 2, 16, 23, 15, 8, 15, 9, 14, 20, 2, 2, 15, 20, 15, 22, 30, 31]\n","Frequência: 1 | Número de grupos: 11\n"]}],"source":["clusters_list_abril = []\n","for i in range(2, 7):\n","  for j in range(2, 7):\n","    clusters_list_abril.append(melhor_agrupamento(df_abril_values, 20000, i, j))\n","clusters_list_abril = np.array(clusters_list_abril)\n","\n","np.save('/content/drive/Shareddrives/datasci4water/IG/data/interim/clusters/som_clusters_list_abril.npy', clusters_list_abril)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I6s491_e-2EN"},"outputs":[],"source":["clusters_list_outubro = []\n","for i in range(2, 7):\n","  for j in range(2, 7):\n","    clusters_list_outubro.append(melhor_agrupamento(df_outubro_values, 20000, i, j))\n","clusters_list_outubro = np.array(clusters_list_outubro)\n","\n","np.save('/content/drive/Shareddrives/datasci4water/IG/data/interim/clusters/som_clusters_list_outubro.npy', clusters_list_outubro)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u8iLMWfKLf_y"},"outputs":[],"source":["clusters_list_abril = np.load('/content/drive/Shareddrives/datasci4water/IG/data/interim/clusters/som_clusters_list_abril.npy')\n","clusters_list_outubro = np.load('/content/drive/Shareddrives/datasci4water/IG/data/interim/clusters/som_clusters_list_outubro.npy')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xC8ym7HgIlUa"},"outputs":[],"source":["def combinacoes_de_matrizes(matriz):\n","  dict_numero_de_clusters = {}\n","  for i in range(matriz.shape[0]):\n","    grupo_str = ' '.join(map(str, matriz[i]))\n","    dict_numero_de_clusters[grupo_str] = len(np.unique(matriz[i]))\n","\n","  quantidade_itens_impressos = 0\n","\n","  # Loop para imprimir os três primeiros itens\n","  # for chave, valor in dict_numero_de_clusters.items():\n","  #     print(\"Agrupamento:\", list(map(int, chave.split())), \"| Número de grupos:\", valor)\n","  #     quantidade_itens_impressos += 1\n","  #     if quantidade_itens_impressos == 3:\n","  #         break\n","\n","  tamanho_combinacoes = len(np.unique(list(dict_numero_de_clusters.values())))\n","  combinacoes = list(itertools.combinations(dict_numero_de_clusters.items(), tamanho_combinacoes))\n","\n","  # Exibindo apenas as combinações que têm valores diferentes\n","  lista_todas_combinacoes = []\n","  for comb in combinacoes:\n","    valores = [valor for chave, valor in comb]\n","    if len(set(valores)) == len(valores):\n","      dicionario_combinado = dict(comb)\n","      dicionario_combinado = list(dicionario_combinado.keys())\n","\n","      for i in range(tamanho_combinacoes):\n","        dicionario_combinado[i] = dicionario_combinado[i].split()\n","        for j in range(matriz.shape[1]):\n","          dicionario_combinado[i][j] = int(dicionario_combinado[i][j])\n","      lista_todas_combinacoes.append(dicionario_combinado)\n","\n","  return (lista_todas_combinacoes)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3zoOrQtvujY9"},"outputs":[],"source":["combinacoes_abril = combinacoes_de_matrizes(clusters_list_abril)\n","combinacoes_abril = np.array(combinacoes_abril)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZwCv5jwh_2MZ"},"outputs":[],"source":["combinacoes_abril[0].shape, df_abril_values.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TNeiNZjA_WOo"},"outputs":[],"source":["# todas_metricas_abril = []\n","# for i in range(combinacoes_abril.shape[0]):\n","  # todas_metricas_abril.append(mtr.print_metrics(combinacoes_abril[0], df_abril_values))\n","mtr.print_metrics(combinacoes_abril[0], df_abril_values)"]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}